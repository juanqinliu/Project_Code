#!/bin/bash

set -e  # Exit on error

# color 
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# script directory
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PYTHON_SCRIPT="$SCRIPT_DIR/onnx_to_engine.py"

# 打印带颜色的消息
print_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

print_step() {
    echo -e "${PURPLE}[STEP]${NC} $1"
}

# Show help information
show_help() {
    echo "Unified TensorRT Engine Export Tool"
    echo ""
    echo "Usage:"
    echo "  $0 [options] -i ONNX_FILE_PATH"
    echo ""
    echo "Required Arguments:"
    echo "  -i, --onnx PATH           Path to the ONNX model file"
    echo ""
    echo "Output Configuration:"
    echo "  --output PATH             Path for the output engine file (auto-generated by default)"
    echo ""
    echo "Precision Configuration:"
    echo "  --precision TYPE          Precision type (fp32/fp16/int8, default: fp32)"
    echo ""
    echo "INT8 Calibration Configuration:"
    echo "  --calib-list PATH         Path to the calibration image list file"
    echo "  --calib-dir PATH          Path to the calibration image directory"
    echo "  --calib-num NUM           Maximum number of calibration images (default: 50)"
    echo "  --calib-cache PATH        Path for the calibration cache file"
    echo ""
    echo "Performance Configuration:"
    echo "  --workspace SIZE          Workspace size in GB (default: 2.0)"
    echo "  --batch-size NUM          Maximum batch size (default: 1)"
    echo "  --input-shape SHAPE       Input shape N,C,H,W (default: 1,3,640,640)"
    echo ""
    echo "DLA Configuration (Jetson only):"
    echo "  --enable-dla              Enable DLA acceleration"
    echo "  --dla-core NUM            DLA core ID (default: 0)"
    echo ""
    echo "Debugging Configuration:"
    echo "  --verbose                 Enable verbose output"
    echo "  --config-file PATH        Load settings from a JSON configuration file"
    echo "  --save-config PATH        Save current configuration to a file"
    echo ""
    echo "Other Options:"
    echo "  -h, --help                Show this help message"
    echo "  --check-env               Check environment dependencies"
    echo "  --list-models             List available ONNX models"
    echo ""
    echo "Usage Examples:"
    echo "  # Basic conversion (FP32)"
    echo "  $0 -i model.onnx"
    echo ""
    echo "  # FP16 precision conversion"
    echo "  $0 -i model.onnx --precision fp16"
    echo ""
    echo "  # INT8 precision conversion (requires calibration data)"
    echo "  $0 -i model.onnx --precision int8 --calib-dir ./calibration_images"
    echo ""
    echo "  # Custom workspace and output path"
    echo "  $0 -i model.onnx --precision fp16 --workspace 4 --output custom.engine"
    echo ""
    echo "  # Using a configuration file"
    echo "  $0 --config-file config.json"
    echo ""
}

# Check environment dependencies
check_environment() {
    print_step "Checking environment dependencies..."
    
    local missing_deps=()
    
    # Check for Python
    if ! command -v python3 &> /dev/null; then
        missing_deps+=("python3")
    else
        local python_version=$(python3 --version 2>&1 | awk '{print $2}')
        print_success "Python version: $python_version"
    fi
    
    # Check for Python script
    if [ ! -f "$PYTHON_SCRIPT" ]; then
        print_error "Python script not found: $PYTHON_SCRIPT"
        return 1
    fi
    
    # Check Python dependencies
    print_info "Checking Python dependencies..."
    
    local python_deps=("tensorrt" "pycuda" "numpy" "opencv-python")
    for dep in "${python_deps[@]}"; do
        if ! python3 -c "import ${dep//-/_}" &> /dev/null; then
            missing_deps+=("python3-$dep")
        else
            print_success "✓ $dep"
        fi
    done
    
    # Check for CUDA
    if command -v nvcc &> /dev/null; then
        local cuda_version=$(nvcc --version | grep "release" | awk '{print $6}' | cut -c2-)
        print_success "CUDA version: $cuda_version"
    else
        print_warning "CUDA (nvcc) not found"
    fi
    
    # Check for GPU
    if command -v nvidia-smi &> /dev/null; then
        local gpu_count=$(nvidia-smi -L | wc -l)
        print_success "Found $gpu_count GPU(s)"
        if [ "$gpu_count" -gt 0 ]; then
            nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits | while read line; do
                print_info "  GPU: $line MB"
            done
        fi
    else
        print_warning "nvidia-smi not found"
    fi
    
    # Report missing dependencies
    if [ ${#missing_deps[@]} -gt 0 ]; then
        print_error "Missing the following dependencies:"
        for dep in "${missing_deps[@]}"; do
            echo "  - $dep"
        done
        print_info "Installation suggestion:"
        echo "  pip3 install tensorrt pycuda numpy opencv-python"
        return 1
    fi
    
    print_success "Environment check passed"
    return 0
}

# List available ONNX models
list_models() {
    print_step "Searching for ONNX model files..."
    
    local search_dirs=("." ".." "../weights" "./weights" "./models")
    local found_models=()
    
    for dir in "${search_dirs[@]}"; do
        if [ -d "$dir" ]; then
            while IFS= read -r -d '' file; do
                found_models+=("$file")
            done < <(find "$dir" -name "*.onnx" -type f -print0 2>/dev/null)
        fi
    done
    
    if [ ${#found_models[@]} -eq 0 ]; then
        print_warning "No ONNX model files found"
        print_info "Searched directories: ${search_dirs[*]}"
        return 1
    fi
    
    print_success "Found ${#found_models[@]} ONNX model files:"
    for model in "${found_models[@]}"; do
        local size=$(du -h "$model" 2>/dev/null | cut -f1 || echo "unknown")
        local abs_path=$(realpath "$model" 2>/dev/null || echo "$model")
        echo "  - $abs_path ($size)"
    done
    
    return 0
}

# Validate ONNX file
validate_onnx_file() {
    local onnx_file="$1"
    
    if [ ! -f "$onnx_file" ]; then
        print_error "ONNX file does not exist: $onnx_file"
        return 1
    fi
    
    if [[ ! "$onnx_file" =~ \.onnx$ ]]; then
        print_error "Incorrect file extension, requires .onnx file"
        return 1
    fi
    
    # Check file size
    local file_size=$(stat -c%s "$onnx_file" 2>/dev/null || stat -f%z "$onnx_file" 2>/dev/null || echo "0")
    if [ "$file_size" -lt 1024 ]; then
        print_warning "ONNX file is too small ($file_size bytes), may not be a valid model file"
    fi
    
    print_success "ONNX file validation passed: $onnx_file"
    return 0
}

# Main function
main() {
    local args=()
    local onnx_file=""
    local check_env_flag=false
    local list_models_flag=false
    
    # Parse command line arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                show_help
                exit 0
                ;;
            --check-env)
                check_env_flag=true
                shift
                ;;
            --list-models)
                list_models_flag=true
                shift
                ;;
            -i|--onnx)
                onnx_file="$2"
                args+=("--onnx" "$2")
                shift 2
                ;;
            --output|--calib-list|--calib-dir|--calib-cache|--config-file|--save-config|--workspace|--batch-size|--input-shape|--calib-num|--dla-core|--precision)
                args+=("$1" "$2")
                shift 2
                ;;
            --fp16|--int8|--enable-dla|--verbose)
                args+=("$1")
                shift
                ;;
            *)
                print_error "Unknown parameter: $1"
                echo "Use --help to view help information"
                exit 1
                ;;
        esac
    done
    
    # Handle environment check
    if [ "$check_env_flag" = true ]; then
        check_environment
        exit $?
    fi
    
    # Handle model listing
    if [ "$list_models_flag" = true ]; then
        list_models
        exit $?
    fi
    
    # Check required parameters
    if [ -z "$onnx_file" ] && [ ${#args[@]} -eq 0 ]; then
        print_error "Missing required parameter -i or --onnx"
        echo ""
        show_help
        exit 1
    fi
    
    # Validate ONNX file
    if [ -n "$onnx_file" ]; then
        if ! validate_onnx_file "$onnx_file"; then
            exit 1
        fi
    fi
    
    # Check environment (simplified version)
    if ! command -v python3 &> /dev/null; then
        print_error "python3 not found"
        exit 1
    fi
    
    if [ ! -f "$PYTHON_SCRIPT" ]; then
        print_error "Python script does not exist: $PYTHON_SCRIPT"
        exit 1
    fi
    
    # Convert precision parameter to Python script format
    local converted_args=()
    local i=0
    while [ $i -lt ${#args[@]} ]; do
        case "${args[$i]}" in
            --precision)
                local precision="${args[$((i+1))]}"
                case "$precision" in
                    fp16)
                        converted_args+=("--fp16")
                        ;;
                    int8)
                        converted_args+=("--int8")
                        ;;
                    fp32)
                        # FP32 is default, no special flag needed
                        ;;
                    *)
                        print_error "Unsupported precision type: $precision"
                        print_error "Supported precision types: fp32, fp16, int8"
                        return 1
                        ;;
                esac
                i=$((i+1))  # Skip the precision value
                ;;
            *)
                # Pass through other arguments
                converted_args+=("${args[$i]}")
                ;;
        esac
        i=$((i+1))
    done
    
    # Execute Python script
    print_step "Starting TensorRT engine build..."
    print_info "Executing command: python3 $PYTHON_SCRIPT ${converted_args[*]}"
    
    # Set environment variables
    export PYTHONPATH="$SCRIPT_DIR:$PYTHONPATH"
    
    # Execute conversion
    if python3 "$PYTHON_SCRIPT" "${converted_args[@]}"; then
        print_success "TensorRT engine build completed!"
        return 0
    else
        print_error "TensorRT engine build failed!"
        return 1
    fi
}

# 脚本入口
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
