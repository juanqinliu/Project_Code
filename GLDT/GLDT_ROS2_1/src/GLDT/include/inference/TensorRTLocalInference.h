#pragma once

#include <string>
#include <vector>
#include <memory>
#include <mutex>
#include <limits>
#include <opencv2/opencv.hpp>
#include <NvInfer.h>
#include "common/Detection.h"
#include "inference/PostprocessCommon.h"  // æ·»åŠ å…±äº«åå¤„ç†å¤´æ–‡ä»¶å¼•ç”¨
#include "inference/local_postprocess.h"  // æ·»åŠ å±€éƒ¨æ£€æµ‹åå¤„ç†å¤´æ–‡ä»¶å¼•ç”¨
#include "inference/InferenceInterface.h"     // æ·»åŠ æ¥å£å¤´æ–‡ä»¶
#include "common/Flags.h"                  // æ·»åŠ Flagså¤´æ–‡ä»¶

#include "inference/preprocess.h" // æ–°å¢preprocess.hå¤´æ–‡ä»¶å¼•ç”¨
#include "common/Logger.h"     // æ·»åŠ Logger.hå¼•ç”¨
#include <fstream>
#include <iostream>
#include <algorithm>
#include <map>
#include <cstring>

namespace tracking {

// TensorRTæ—¥å¿—è®°å½•å™¨ï¼ˆå±€éƒ¨ï¼‰
class LocalLogger : public nvinfer1::ILogger {
public:
    void log(Severity severity, const char* msg) noexcept override {
        if (severity <= Severity::kWARNING) {
            std::cout << "[TensorRT Local] " << msg << std::endl;
        }
    }
};

// æ·»åŠ ç»§æ‰¿è‡ªInferenceInterface
class TensorRTLocalInference : public InferenceInterface {
public:
    TensorRTLocalInference(const std::string& engine_path);
    TensorRTLocalInference(const std::string& engine_path, int preprocess_mode);
    ~TensorRTLocalInference() override;

    // å•å¼ å›¾åƒæ£€æµ‹ - é‡å†™åŸºç±»æ–¹æ³•
    std::vector<Detection> detect(const cv::Mat& image, float conf_threshold = 0.5f) override;
    
    // æ‰¹é‡å›¾åƒæ£€æµ‹ - é‡å†™åŸºç±»æ–¹æ³•
    std::vector<std::vector<Detection>> detectBatch(
        const std::vector<cv::Mat>& images, float conf_threshold = 0.5f) override;
    
    // è®¾ç½®åå¤„ç†æ¨¡å¼
    void setPostprocessMode(PostprocessMode mode) { postprocess_mode_ = mode; }
    
    // è·å–å½“å‰åå¤„ç†æ¨¡å¼
    PostprocessMode getPostprocessMode() const { return postprocess_mode_; }
    
    // ROIåŒºåŸŸæ£€æµ‹
    std::vector<Detection> detectROIs(const std::vector<cv::Mat>& rois, float conf_threshold = 0.5f);
    
    // æ£€æŸ¥æ˜¯å¦æ”¯æŒæ‰¹é‡æ£€æµ‹ - é‡å†™åŸºç±»æ–¹æ³•
    bool supportsBatchDetection() const override;
    
    // è·å–æœ€å¤§æ‰¹æ¬¡å¤§å° - é‡å†™åŸºç±»æ–¹æ³•
    int getMaxBatchSize() const override;
    
    // ä»Flagsè·å–å±€éƒ¨é¢„å¤„ç†æ¨¡å¼
    static int getLocalPreprocessMode();
    
    // ä»Flagsè·å–å±€éƒ¨åå¤„ç†æ¨¡å¼
    static PostprocessMode getLocalPostprocessMode();
    
    // ğŸ”¥ å±€éƒ¨æ¨ç†æ—¶é—´ç»Ÿè®¡æ–¹æ³•
    void updateLocalPreprocessTime(double preprocess_time);
    void updateLocalInferenceTime(double inference_time);
    void updateLocalPostprocessTime(double postprocess_time);
    void updateLocalTotalTime(double total_time);
    void printLocalTimeStatistics();

private:
    // åˆå§‹åŒ–ç»‘å®š
    bool initializeBindings();
    
    // è·å–ç»‘å®šå°ºå¯¸
    size_t getBindingSize(const nvinfer1::Dims& dims);
    
    // è°ƒè¯•ç»‘å®š
    void debugBindings();
    
    // æ‰§è¡Œæ‰¹é‡æ¨ç†
    std::vector<std::vector<Detection>> executeBatchInference(
        const std::vector<cv::Mat>& batch_images, float conf_threshold);
    
    // æ‰¹é‡é¢„å¤„ç†
    void preprocessBatch(const std::vector<cv::Mat>& images, float* batch_input_device);
    
    // æ‰¹é‡åå¤„ç†ï¼ˆCPUç‰ˆæœ¬ï¼‰
    std::vector<std::vector<Detection>> postprocessBatch(
        float* batch_output, 
        const std::vector<cv::Mat>& original_images, 
        float conf_threshold);
    
    // æ‰¹é‡YOLOè§£ç ï¼ˆCPUç‰ˆæœ¬ï¼‰
    void batchDecodeYOLOOutput(
        float* batch_output, 
        int batch_size,
        const std::vector<cv::Mat>& original_images,
        std::vector<std::vector<Detection>>& batch_detections,
        float conf_threshold);

    // é¢„å¤„ç†ç›¸å…³æ–¹æ³•
    void preprocessImageCPU(const cv::Mat& image, float* input_device_buffer);
    void preprocessImageCVAffine(const cv::Mat& image, float* input_device_buffer);
    void preprocessImageGPU(const cv::Mat& image, float* input_device_buffer);
    
    // è¾…åŠ©å‡½æ•°
    cv::Mat letterbox(const cv::Mat& src);
    void preprocessImage(const cv::Mat& image, float* input_data);
    float clamp(const float val, const float minVal, const float maxVal);
    cv::Rect get_rect(const cv::Mat& img, float bbox[4]);
    
    // CPUåå¤„ç†å‡½æ•°
    std::vector<Detection> parseYOLOOutput(float* output, int output_size, 
                                     const cv::Mat& original_image, 
                                     float conf_threshold);
    float iou(float lbox[4], float rbox[4]);
    void nms(std::vector<Detection>& res, float* output, float conf_thresh, float nms_thresh);
    
    // ä¸Šä¸‹æ–‡åˆå§‹åŒ–
    bool initPreprocessContext();
    bool ensurePreprocessBuffer(size_t required_size);
    bool initBatchContext();
    bool ensureBatchBuffers(int batch_size);
    
    // TensorRTç›¸å…³
    std::unique_ptr<nvinfer1::IRuntime> runtime_;
    std::unique_ptr<nvinfer1::ICudaEngine> engine_;
    std::unique_ptr<nvinfer1::IExecutionContext> context_;
    std::vector<void*> bindings_;
    cudaStream_t stream_;
    int active_profile_index_ = 0;  // ğŸ”¥ Store active optimization profile index
    
    // é¢„å¤„ç†å’Œæ‰¹å¤„ç†é…ç½®
    int preprocess_mode_;
    bool cuda_initialized_;
    bool bindings_initialized_;
    bool needs_dynamic_setting_ = false;
    
    // åå¤„ç†é…ç½®
    PostprocessMode postprocess_mode_ = getLocalPostprocessMode();
    
    // æ¨¡å‹é…ç½®
    cv::Size input_dims_;
    int input_size_;
    int output_size_;
    int num_boxes_;
    int detection_output_index_;
    
    // é¢„å¤„ç†ä¸Šä¸‹æ–‡
    struct PreprocessContext {
        cudaStream_t stream = nullptr;
        void* device_buffer = nullptr;
        size_t buffer_capacity = 0;
        std::mutex mutex;
        
        ~PreprocessContext() {
            if (stream) cudaStreamDestroy(stream);
            if (device_buffer) cudaFree(device_buffer);
        }
    };
    std::unique_ptr<PreprocessContext> preprocess_ctx_;
    
    // æ‰¹å¤„ç†å†…å­˜ä¸Šä¸‹æ–‡
    struct BatchMemoryContext {
        void* input_buffer = nullptr;
        void* output_buffer = nullptr;
        size_t input_capacity = 0;
        size_t output_capacity = 0;
        int max_batch_allocated = 0;
        std::mutex mutex;
        
        ~BatchMemoryContext() {
            if (input_buffer) cudaFree(input_buffer);
            if (output_buffer) cudaFree(output_buffer);
        }
    };
    std::unique_ptr<BatchMemoryContext> batch_ctx_;
    
    // æ¨¡å‹å¸¸é‡
    static constexpr int kInputW = 640;
    static constexpr int kInputH = 640;
    static constexpr int kBoxInfoSize = 5;  // x, y, w, h, conf
    
    // ğŸ”¥ å±€éƒ¨æ¨ç†æ—¶é—´ç»Ÿè®¡æˆå‘˜å˜é‡
    double total_preprocess_time_ = 0.0;
    double avg_preprocess_time_ = 0.0;
    double max_preprocess_time_ = 0.0;
    double min_preprocess_time_ = std::numeric_limits<double>::max();
    int preprocess_count_ = 0;
    
    double total_inference_time_ = 0.0;
    double avg_inference_time_ = 0.0;
    double max_inference_time_ = 0.0;
    double min_inference_time_ = std::numeric_limits<double>::max();
    int inference_count_ = 0;
    
    double total_postprocess_time_ = 0.0;
    double avg_postprocess_time_ = 0.0;
    double max_postprocess_time_ = 0.0;
    double min_postprocess_time_ = std::numeric_limits<double>::max();
    int postprocess_count_ = 0;
    
    double total_processing_time_ = 0.0;
    double avg_total_time_ = 0.0;
    double max_total_time_ = 0.0;
    double min_total_time_ = std::numeric_limits<double>::max();
};

} // namespace tracking 